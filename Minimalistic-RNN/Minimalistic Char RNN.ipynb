{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'alice.txt'\n",
    "data = open(filename, encoding=\"UTF-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char =  list(set(data))   ## Number of Different Chracters in Library\n",
    "vocab_length = len(char)  ## Length of the vocabulary\n",
    "data_length =  len(data)  ## Length of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Mapping characters in vocabulary to Integer Index and vice_versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int = {ch : i for i,ch in enumerate(char)}\n",
    "int_char = {i:ch for i,ch in enumerate(char)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 128 # Size of Hidden Layer \n",
    "sequence= 25 # Number of steps to unroll the RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wax,Was,Way,b1,b2 are the parameters of RNN. Equation for forward pass is below:\n",
    "$$ s = tanh(W_{ax}.x + W_{as}.s_{t-1} + b1) $$\n",
    "$$ y = softmax(W_{ay}.s{t} + b2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wax = np.random.randn(hidden, vocab_length) * 0.01\n",
    "Was = np.random.randn(hidden,hidden) * 0.01\n",
    "Way = np.random.randn(vocab_length,hidden) * 0.01\n",
    "b1 = np.zeros((hidden,1))\n",
    "b2 = np.zeros((vocab_length,1))\n",
    "initial_hidden = np.zeros((hidden,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training and Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLoss(inputs, labels, initial_hidden):\n",
    "    '''\n",
    "    This function calculates the loss and performs \n",
    "    backpropagation in time. The loss function is \n",
    "    cross-entropy loss function. It also returns the \n",
    "    gradients of RNN paramters which is to be used \n",
    "    by optimizer.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : List of Integers. Each element in list corresponds to a character's \n",
    "             integer value, which is determined by char_int dictionary. \n",
    "\n",
    "    labels : True Labels corresponding to inputs.It is next character \n",
    "             to input in training data.\n",
    "             It is also List of Integers where integer mapping is determined as for inputs.\n",
    "             \n",
    "    initial_hidden: An initial state given to RNN.\n",
    "             \n",
    "    Returns\n",
    "    -------\n",
    "    loss   : The cross entropy loss on inputs\n",
    "    dwax   : Gradient of loss wrt parameter Wax.\n",
    "    dwas   : Gradient of loss wrt parameter Was.\n",
    "    dway   : Gradient of loss wrt parameter Way.\n",
    "    db1    : Gradient of loss wrt parameter b1.\n",
    "    db2    : Gradient of loss wrt parameter b2.\n",
    "    \n",
    "    s[len(inputs)-1] : \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## These dictionaries hold various activation at time ##\n",
    "    \n",
    "    x = {}  ##  Holds input characters as one hot vectors  \n",
    "    s = {}  ## Ouput Activation at Layer 1 \n",
    "    y = {} ## Output from softmax \n",
    "    a1 ={} ## Input Preactivation at Layer 1 \n",
    "    a2= {}  ## Input Preactivation at Layer 2\n",
    "    \n",
    "    s[-1] = initial_hidden\n",
    "    loss = 0\n",
    "    \n",
    "    ## Forward Propagate \n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "        \n",
    "        ## Encode to one hot vector to give x\n",
    "        ## dimensions of x[t] = vocab_length * 1\n",
    "        x[t] = np.zeros((vocab_length,1))\n",
    "        x[t][inputs[t]] = 1\n",
    "        \n",
    "        ## Calculate preactivation at Layer 1 (Input Layer)\n",
    "        ## dimensions of a1 = (hidden,1)\n",
    "        a1[t] =  np.dot(Wax, x[t]) + np.dot(Was,s[t-1]) + b1\n",
    "        \n",
    "        ## Calculate activation at Layer 1\n",
    "        ## dimensions of s1 =  (hidden,1)\n",
    "        s[t]  =  np.tanh(a1[t])    \n",
    "        \n",
    "        ## Input Layer Preactivation at Layer 2\n",
    "        ## dimensions of a2 =  (vocab_length,1)\n",
    "        a2[t] =  np.dot(Way, s[t]) + b2\n",
    "        \n",
    "        ## Output of RNN (softmax function)\n",
    "        ## dimensions of y =  (vocab_length,1)\n",
    "        y[t] =   np.exp(a2[t])/np.sum(np.exp(a2[t]))\n",
    "        \n",
    "        ## Cross Entropy Loss\n",
    "        loss += -np.log(y[t][labels[t],0]) \n",
    "        \n",
    "    ### Backpropagation in Time\n",
    "    \n",
    "    #\n",
    "    dwas = np.zeros_like(Was) \n",
    "    dway = np.zeros_like(Way)\n",
    "    dwax = np.zeros_like(Wax)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    db2 = np.zeros_like(b2)\n",
    "    dsnext = np.zeros_like(s[0]) ## This is the derivative component for s from layer ahead in time.\n",
    "    \n",
    "    for t in range(len(inputs)-1,-1,-1):\n",
    "        ## Converted true label to one hot vector\n",
    "        e = np.zeros_like(y[0])\n",
    "        e[labels[t]] = 1\n",
    "        \n",
    "        ## Backpropgate through softmax\n",
    "        e = (y[t]-e)\n",
    "        \n",
    "        ## Compute gradients for Way and b2\n",
    "        dway +=  np.dot(e, s[t].T)\n",
    "        db2  +=  e\n",
    "        \n",
    "        ## Calculate Gradient of output activation\n",
    "        ## of layer 1 (s). Also, add the gradient\n",
    "        ## coming from previous step.\n",
    "        ds = np.dot(Way.T, e) + dsnext\n",
    "        \n",
    "        ## Backpropgate into tanh non-linearlity\n",
    "        ## Gradient wrt to input preactivation at layer 1\n",
    "        da1 = (1-s[t]*s[t])*ds\n",
    "        \n",
    "        ### Gradient wrt to Wax, Was, b1\n",
    "        dwax +=  np.dot(da1, x[t].T)\n",
    "        db1  +=  da1\n",
    "        dwas += np.dot(da1, s[t-1].T)\n",
    "        \n",
    "        ### Calculate gradient wrt to s{t-1} at time step\n",
    "        ## t. This will be added to s{t-1} at time step t-1\n",
    "        ## More evident from drawing computation graph.\n",
    "        dsnext =  np.dot(Was.T, da1)\n",
    "    \n",
    "    ### Mitigating the exploding gradients problem\n",
    "    ### See Pascanu, R., Mikolov, T., & Bengio, Y. (2013, February). \n",
    "    ## On the difficulty of training recurrent neural networks(ICML).\n",
    "    for dparam in [dwax,dwas,dway,db1,db2]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) \n",
    "  \n",
    "    return loss,dwax,dwas,dway,db1,db2,s[len(inputs)-1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(s, seed_ix, n):\n",
    "    \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "        Runs the RNN in forward mode for n steps. Returns\n",
    "        the sequence of characters generated by RNN\n",
    "        \n",
    "        Parameters\n",
    "        -------------\n",
    "        n       : The number of steps to run RNN for\n",
    "        seed_ix : The initial seed to start generating characters from\n",
    "        s       : Initial memory state\n",
    "        \n",
    "        Returns\n",
    "        -------------\n",
    "        char_seq  : An array of Integers, which corresponds to character sequence\n",
    "                  generated by RNN. The characters can be retrieved from \n",
    "                  int_char dictionary defined earlier in data processing section\n",
    "        \n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_length, 1))\n",
    "    x[seed_ix] = 1\n",
    "    char_seq = []\n",
    "\n",
    "    for t in range(n):\n",
    "        # Run the forward pass only.\n",
    "        s = np.tanh(np.dot(Wax, x) + np.dot(Was, s) + b1)\n",
    "        a2 = np.dot(Way, s) + b2\n",
    "        y = np.exp(a2) / np.sum(np.exp(a2))\n",
    "\n",
    "        # Sample from the distribution produced by softmax.\n",
    "        ix = np.random.choice(range(vocab_length), p=y.ravel())\n",
    "\n",
    "        # Prepare input for the next cell.\n",
    "        x = np.zeros((vocab_length, 1))\n",
    "        x[ix] = 1\n",
    "        char_seq.append(ix)\n",
    "    return char_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCheck(inputs, targets, hprev):\n",
    "  global Wax, Was, Way, b1, b2\n",
    "  num_checks, delta = 10, 1e-5\n",
    "  _, dwax,dwas,dway,db1,db2,_ = calculateLoss(inputs, targets, hprev)\n",
    "  for param,dparam,name in zip([ Wax, Was, Way, b1, b2],\n",
    "                               [dwax,dwas,dway,db1,db2],\n",
    "                               ['Wax', 'Was', 'Way', 'b1', 'b2']):\n",
    "    s0 = dparam.shape\n",
    "    s1 = param.shape\n",
    "    assert s0 == s1, 'Error dims dont match: %s and %s.' % (s0, s1)\n",
    "    print(name)\n",
    "    for i in range(num_checks):\n",
    "      ri = int(uniform(0,param.size))\n",
    "      # evaluate cost at [x + delta] and [x - delta]\n",
    "      old_val = param.flat[ri]\n",
    "      param.flat[ri] = old_val + delta\n",
    "      cg0, _, _, _, _, _,_ = calculateLoss(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val - delta\n",
    "      cg1, _, _, _, _, _,_ = calculateLoss(inputs, targets, hprev)\n",
    "      param.flat[ri] = old_val # reset old value for this parameter\n",
    "      # fetch both numerical and analytic gradient\n",
    "      grad_analytic = dparam.flat[ri]\n",
    "      grad_numerical = (cg0 - cg1) / ( 2 * delta )\n",
    "      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "      print('%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))\n",
    "      # rel_error should be on order of 1e-7 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicGradCheck():\n",
    "    inputs = [char_int[ch] for ch in data[:sequence]]\n",
    "    targets = [char_int[ch] for ch in data[1:sequence+1]]\n",
    "    hprev = np.zeros((hidden,1)) # reset RNN memory\n",
    "    gradCheck(inputs, targets, hprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wax\n",
      "0.000000, 0.000000 => nan \n",
      "0.025383, 0.025383 => 1.628963e-08 \n",
      "0.000000, 0.000000 => nan \n",
      "0.000000, 0.000000 => nan \n",
      "0.000000, 0.000000 => nan \n",
      "0.000000, 0.000000 => nan \n",
      "0.000000, 0.000000 => nan "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.001834, -0.001834 => 1.880710e-07 \n",
      "0.000000, 0.000000 => nan \n",
      "0.000000, 0.000000 => nan \n",
      "Was\n",
      "0.000042, 0.000042 => 1.490677e-05 \n",
      "0.000705, 0.000705 => 2.742790e-07 \n",
      "-0.000104, -0.000104 => 4.299940e-07 \n",
      "-0.000186, -0.000186 => 2.516666e-06 \n",
      "-0.000283, -0.000283 => 1.690710e-06 \n",
      "-0.000023, -0.000023 => 2.417268e-05 \n",
      "-0.000626, -0.000626 => 2.619935e-07 \n",
      "0.000150, 0.000150 => 2.233340e-06 \n",
      "0.000699, 0.000699 => 9.612677e-07 \n",
      "-0.000266, -0.000266 => 4.967051e-07 \n",
      "Way\n",
      "-0.000684, -0.000684 => 1.276520e-06 \n",
      "-0.000058, -0.000058 => 6.601337e-06 \n",
      "0.000573, 0.000573 => 9.821675e-07 \n",
      "-0.014387, -0.014387 => 2.236810e-08 \n",
      "-0.000197, -0.000197 => 3.601451e-06 \n",
      "0.000014, 0.000014 => 1.654030e-05 \n",
      "0.000033, 0.000033 => 1.649125e-06 \n",
      "0.008641, 0.008641 => 9.250453e-09 \n",
      "-0.000229, -0.000229 => 1.562184e-06 \n",
      "-0.015704, -0.015704 => 5.711710e-08 \n",
      "b1\n",
      "-0.071859, -0.071859 => 5.806325e-09 \n",
      "-0.043646, -0.043646 => 2.389464e-08 \n",
      "0.032986, 0.032986 => 7.768636e-09 \n",
      "0.004281, 0.004281 => 1.823768e-07 \n",
      "-0.041991, -0.041991 => 8.146992e-09 \n",
      "-0.008326, -0.008326 => 2.227370e-08 \n",
      "-0.107077, -0.107077 => 4.433727e-10 \n",
      "-0.100505, -0.100505 => 5.245833e-09 \n",
      "-0.055318, -0.055318 => 9.821008e-09 \n",
      "0.103860, 0.103860 => 1.157954e-09 \n",
      "b2\n",
      "0.347361, 0.347361 => 2.746313e-09 \n",
      "0.347395, 0.347395 => 1.554822e-09 \n",
      "0.347172, 0.347172 => 6.599264e-10 \n",
      "0.347349, 0.347349 => 3.163624e-09 \n",
      "-0.652717, -0.652717 => 2.063692e-10 \n",
      "0.347237, 0.347237 => 1.354538e-10 \n",
      "-1.652981, -1.652981 => 3.453928e-10 \n",
      "0.347138, 0.347138 => 1.092232e-10 \n",
      "-0.652896, -0.652896 => 5.845500e-10 \n",
      "0.347245, 0.347245 => 3.926294e-10 \n"
     ]
    }
   ],
   "source": [
    "basicGradCheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " o sttonglpet, iPk gelee doike. ‘Hould’l theen’s Tat thr ire veh ig ingh yout ind tut ases the\n",
      "bercund chisew toimy ange Halget.’\n",
      "\n",
      "E:’r ars!’\n",
      "\n",
      "rle, waiky rot, and rreruwedy shel ur thedr C!’ \n",
      "Le sald e \n",
      "----\n",
      "iter 0 (p=0), loss: 106.932519\n",
      "iter 500 (p=12500), loss: 94.465643\n",
      "iter 1000 (p=25000), loss: 82.885664\n",
      "iter 1500 (p=37500), loss: 74.838225\n",
      "iter 2000 (p=50000), loss: 69.646248\n",
      "iter 2500 (p=62500), loss: 65.651194\n",
      "iter 3000 (p=75000), loss: 62.760084\n",
      "iter 3500 (p=87500), loss: 61.152050\n",
      "iter 4000 (p=100000), loss: 59.678742\n",
      "iter 4500 (p=112500), loss: 59.171045\n",
      "iter 5000 (p=125000), loss: 58.906037\n",
      "iter 5500 (p=137500), loss: 58.080072\n",
      "iter 6000 (p=5600), loss: 58.091562\n",
      "iter 6500 (p=18100), loss: 57.399624\n",
      "iter 7000 (p=30600), loss: 56.919954\n",
      "iter 7500 (p=43100), loss: 56.850266\n",
      "iter 8000 (p=55600), loss: 56.287581\n",
      "iter 8500 (p=68100), loss: 56.018038\n",
      "iter 9000 (p=80600), loss: 55.346352\n",
      "iter 9500 (p=93100), loss: 55.233367\n",
      "----\n",
      " she gyoutle fore, in tvi: he.\n",
      "rforncut to kare to peren I hy of ithen mund ance sumte, owed bace on of?”’\n",
      "\n",
      "‘Bout noqu esteneg ar ther’y” sasas wargouch rigtid\n",
      "suln to horfor in nog lfret warvens, that \n",
      "----\n",
      "iter 10000 (p=105600), loss: 54.691419\n",
      "iter 10500 (p=118100), loss: 54.915506\n",
      "iter 11000 (p=130600), loss: 54.907352\n",
      "iter 11500 (p=143100), loss: 54.980104\n",
      "iter 12000 (p=11200), loss: 54.927856\n",
      "iter 12500 (p=23700), loss: 54.912624\n",
      "iter 13000 (p=36200), loss: 54.770569\n",
      "iter 13500 (p=48700), loss: 54.786407\n",
      "iter 14000 (p=61200), loss: 54.462063\n",
      "iter 14500 (p=73700), loss: 53.767524\n",
      "iter 15000 (p=86200), loss: 53.434250\n",
      "iter 15500 (p=98700), loss: 53.351566\n",
      "iter 16000 (p=111200), loss: 53.517028\n",
      "iter 16500 (p=123700), loss: 53.929679\n",
      "iter 17000 (p=136200), loss: 53.534879\n",
      "iter 17500 (p=4300), loss: 54.112480\n",
      "iter 18000 (p=16800), loss: 53.832558\n",
      "iter 18500 (p=29300), loss: 53.790004\n",
      "iter 19000 (p=41800), loss: 54.017013\n",
      "iter 19500 (p=54300), loss: 53.549013\n",
      "----\n",
      " \n",
      "‘Yat it saith tupit, as at hy at thist?’\n",
      "\n",
      "‘I ing fin.) she poo fors on ullipe hintthe dre\n",
      "tored thery, Aling ly tollotly she nom the, andencenver coullithich whur is ther tomed somey themp hath mnate \n",
      "----\n",
      "iter 20000 (p=66800), loss: 53.421756\n",
      "iter 20500 (p=79300), loss: 52.831648\n",
      "iter 21000 (p=91800), loss: 52.844131\n",
      "iter 21500 (p=104300), loss: 52.422619\n",
      "iter 22000 (p=116800), loss: 52.522976\n",
      "iter 22500 (p=129300), loss: 52.696769\n",
      "iter 23000 (p=141800), loss: 52.907782\n",
      "iter 23500 (p=9900), loss: 53.003461\n",
      "iter 24000 (p=22400), loss: 53.151492\n",
      "iter 24500 (p=34900), loss: 53.182974\n",
      "iter 25000 (p=47400), loss: 53.223681\n",
      "iter 25500 (p=59900), loss: 52.935894\n",
      "iter 26000 (p=72400), loss: 52.484729\n",
      "iter 26500 (p=84900), loss: 52.007126\n",
      "iter 27000 (p=97400), loss: 52.051159\n",
      "iter 27500 (p=109900), loss: 52.197621\n",
      "iter 28000 (p=122400), loss: 52.595195\n",
      "iter 28500 (p=134900), loss: 52.096364\n",
      "iter 29000 (p=3000), loss: 52.583255\n",
      "iter 29500 (p=15500), loss: 52.601369\n",
      "----\n",
      " emot of fo t, as ollitked in hou\n",
      "sasl\n",
      "tes!’ hanGok frad mon of dir she rther Mand thils bnoth cean to notteinte beche wair it it nottlich Mat sor in. )a wouther the sistl whey fole: azle thane lentert \n",
      "----\n",
      "iter 30000 (p=28000), loss: 52.856143\n",
      "iter 30500 (p=40500), loss: 52.963035\n",
      "iter 31000 (p=53000), loss: 52.557171\n",
      "iter 31500 (p=65500), loss: 52.491957\n",
      "iter 32000 (p=78000), loss: 51.832161\n",
      "iter 32500 (p=90500), loss: 51.855403\n",
      "iter 33000 (p=103000), loss: 51.631347\n",
      "iter 33500 (p=115500), loss: 51.619530\n",
      "iter 34000 (p=128000), loss: 51.837532\n",
      "iter 34500 (p=140500), loss: 52.024251\n",
      "iter 35000 (p=8600), loss: 52.312669\n",
      "iter 35500 (p=21100), loss: 52.228825\n",
      "iter 36000 (p=33600), loss: 52.396526\n",
      "iter 36500 (p=46100), loss: 52.346500\n",
      "iter 37000 (p=58600), loss: 52.150272\n",
      "iter 37500 (p=71100), loss: 51.739858\n",
      "iter 38000 (p=83600), loss: 51.287193\n",
      "iter 38500 (p=96100), loss: 51.377880\n",
      "iter 39000 (p=108600), loss: 51.232857\n",
      "iter 39500 (p=121100), loss: 51.382631\n",
      "----\n",
      " pice to sarte, of in eet, Thar,\n",
      "eattim,\n",
      "Tok pyabf shing the’m.\n",
      " ‘Whary ce taed wevered seainged\n",
      "yAlice,\n",
      "Fit bey not’ the gaser nout ou the hot it nemoung rougned che ppath!’\n",
      "\n",
      "‘One ony. ‘e!’\n",
      "phow wicat \n",
      "----\n",
      "iter 40000 (p=133600), loss: 51.374926\n",
      "iter 40500 (p=1700), loss: 51.747455\n",
      "iter 41000 (p=14200), loss: 51.753660\n",
      "iter 41500 (p=26700), loss: 52.283208\n",
      "iter 42000 (p=39200), loss: 52.081367\n",
      "iter 42500 (p=51700), loss: 52.050106\n",
      "iter 43000 (p=64200), loss: 51.799870\n",
      "iter 43500 (p=76700), loss: 51.121011\n",
      "iter 44000 (p=89200), loss: 51.217509\n",
      "iter 44500 (p=101700), loss: 51.082964\n",
      "iter 45000 (p=114200), loss: 51.065146\n",
      "iter 45500 (p=126700), loss: 51.248750\n",
      "iter 46000 (p=139200), loss: 51.177810\n",
      "iter 46500 (p=7300), loss: 51.580484\n",
      "iter 47000 (p=19800), loss: 51.555800\n",
      "iter 47500 (p=32300), loss: 51.830054\n",
      "iter 48000 (p=44800), loss: 51.842740\n",
      "iter 48500 (p=57300), loss: 51.509543\n",
      "iter 49000 (p=69800), loss: 51.345747\n",
      "iter 49500 (p=82300), loss: 50.802971\n",
      "----\n",
      " t leothress at a llachth (phish\n",
      "satith an the way how the sater. ‘I and theveen.\n",
      "\n",
      "‘C En-YoS and they satting y I toinked, erecl iteod to daldouge me to graed saring sailf digene fiw, lome reens\n",
      "spowci \n",
      "----\n",
      "iter 50000 (p=94800), loss: 50.932127\n",
      "iter 50500 (p=107300), loss: 50.612370\n",
      "iter 51000 (p=119800), loss: 50.687280\n",
      "iter 51500 (p=132300), loss: 50.665512\n",
      "iter 52000 (p=400), loss: 51.016272\n",
      "iter 52500 (p=12900), loss: 51.235399\n",
      "iter 53000 (p=25400), loss: 51.826124\n",
      "iter 53500 (p=37900), loss: 51.535436\n",
      "iter 54000 (p=50400), loss: 51.653470\n",
      "iter 54500 (p=62900), loss: 51.220880\n",
      "iter 55000 (p=75400), loss: 50.686938\n",
      "iter 55500 (p=87900), loss: 50.703229\n",
      "iter 56000 (p=100400), loss: 50.592226\n",
      "iter 56500 (p=112900), loss: 50.519081\n",
      "iter 57000 (p=125400), loss: 50.789602\n",
      "iter 57500 (p=137900), loss: 50.543784\n",
      "iter 58000 (p=6000), loss: 51.196410\n",
      "iter 58500 (p=18500), loss: 51.069303\n",
      "iter 59000 (p=31000), loss: 51.302762\n",
      "iter 59500 (p=43500), loss: 51.505282\n",
      "----\n",
      "  mor\n",
      "thice\n",
      "N‘AEchich gigel, abese frought laid bat waince inine a vey welleo udAppi-ch douns,’ to Ale s forore; ‘now me tike\n",
      "caly rese.\n",
      "\n",
      "‘OR*  nat in it patt the buts hey purredinstly.\n",
      "\n",
      "‘Or\n",
      "alm.’ man, \n",
      "----\n",
      "iter 60000 (p=56000), loss: 51.107210\n",
      "iter 60500 (p=68500), loss: 50.985125\n",
      "iter 61000 (p=81000), loss: 50.419539\n",
      "iter 61500 (p=93500), loss: 50.547078\n",
      "iter 62000 (p=106000), loss: 50.111451\n",
      "iter 62500 (p=118500), loss: 50.179724\n",
      "iter 63000 (p=131000), loss: 50.290907\n",
      "iter 63500 (p=143500), loss: 50.500423\n",
      "iter 64000 (p=11600), loss: 50.676387\n",
      "iter 64500 (p=24100), loss: 51.171413\n",
      "iter 65000 (p=36600), loss: 51.169743\n",
      "iter 65500 (p=49100), loss: 51.231911\n",
      "iter 66000 (p=61600), loss: 50.969416\n",
      "iter 66500 (p=74100), loss: 50.372930\n",
      "iter 67000 (p=86600), loss: 50.265890\n",
      "iter 67500 (p=99100), loss: 50.184093\n",
      "iter 68000 (p=111600), loss: 50.196315\n",
      "iter 68500 (p=124100), loss: 50.423609\n",
      "iter 69000 (p=136600), loss: 50.170562\n",
      "iter 69500 (p=4700), loss: 50.852177\n",
      "----\n",
      " o she maid ther upe bovere the dore to forme of AnIt\n",
      "  su ithisvonkin and for plannt spied in\n",
      "thing onis--feake, dthe wot AhSteid!’\n",
      "\n",
      "‘I’d garsing\n",
      "ipser yoned; bownot was erian gostice hu\n",
      "pell eagel of \n",
      "----\n",
      "iter 70000 (p=17200), loss: 50.696126\n",
      "iter 70500 (p=29700), loss: 50.830567\n",
      "iter 71000 (p=42200), loss: 51.235278\n",
      "iter 71500 (p=54700), loss: 50.709565\n",
      "iter 72000 (p=67200), loss: 50.676819\n",
      "iter 72500 (p=79700), loss: 50.188571\n",
      "iter 73000 (p=92200), loss: 50.243814\n",
      "iter 73500 (p=104700), loss: 49.773324\n",
      "iter 74000 (p=117200), loss: 49.696763\n",
      "iter 74500 (p=129700), loss: 49.889762\n",
      "iter 75000 (p=142200), loss: 50.049393\n",
      "iter 75500 (p=10300), loss: 50.393408\n",
      "iter 76000 (p=22800), loss: 50.697308\n",
      "iter 76500 (p=35300), loss: 50.780947\n",
      "iter 77000 (p=47800), loss: 50.843250\n",
      "iter 77500 (p=60300), loss: 50.582978\n",
      "iter 78000 (p=72800), loss: 50.085795\n",
      "iter 78500 (p=85300), loss: 49.757373\n",
      "iter 79000 (p=97800), loss: 49.762107\n",
      "iter 79500 (p=110300), loss: 49.771491\n",
      "----\n",
      " helcere bat?’ Alins hing fordy wroun ashted fy a eve chouldiot leaw for. ‘I she Cabyo,\n",
      "oout?’\n",
      "wnried of wes witrod’t hast wurw.’ yous uce, af paick arcey it noot ith: of an for\n",
      "to salling the lime.\n",
      "\n",
      "‘ \n",
      "----\n",
      "iter 80000 (p=122800), loss: 50.101729\n",
      "iter 80500 (p=135300), loss: 49.714629\n",
      "iter 81000 (p=3400), loss: 50.249546\n",
      "iter 81500 (p=15900), loss: 50.372380\n",
      "iter 82000 (p=28400), loss: 50.632380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 82500 (p=40900), loss: 50.926881\n",
      "iter 83000 (p=53400), loss: 50.428268\n",
      "iter 83500 (p=65900), loss: 50.351247\n",
      "iter 84000 (p=78400), loss: 49.741620\n",
      "iter 84500 (p=90900), loss: 49.800039\n",
      "iter 85000 (p=103400), loss: 49.495639\n",
      "iter 85500 (p=115900), loss: 49.369346\n",
      "iter 86000 (p=128400), loss: 49.602021\n",
      "iter 86500 (p=140900), loss: 49.760786\n",
      "iter 87000 (p=9000), loss: 50.164074\n",
      "iter 87500 (p=21500), loss: 50.273807\n",
      "iter 88000 (p=34000), loss: 50.477719\n",
      "iter 88500 (p=46500), loss: 50.532039\n",
      "iter 89000 (p=59000), loss: 50.293374\n",
      "iter 89500 (p=71500), loss: 49.782542\n",
      "----\n",
      " s.\n",
      "\n",
      "‘It quen,\n",
      "litthing, to-’\n",
      "\n",
      "‘Jur wis the bebuthen dou nignink she catch sell, ‘Bhe: for\n",
      "Hat’r taid Alar coup.\n",
      "\n",
      "ATines eve wat a seam, a mo, hare trilge sheme Bo,’\n",
      "soane nowe: ‘Oofthy tooklirmome cea \n",
      "----\n",
      "iter 90000 (p=84000), loss: 49.391592\n",
      "iter 90500 (p=96500), loss: 49.487886\n",
      "iter 91000 (p=109000), loss: 49.327503\n",
      "iter 91500 (p=121500), loss: 49.541688\n",
      "iter 92000 (p=134000), loss: 49.364653\n",
      "iter 92500 (p=2100), loss: 49.722678\n",
      "iter 93000 (p=14600), loss: 49.859658\n",
      "iter 93500 (p=27100), loss: 50.502665\n",
      "iter 94000 (p=39600), loss: 50.421744\n",
      "iter 94500 (p=52100), loss: 50.264139\n",
      "iter 95000 (p=64600), loss: 50.083433\n",
      "iter 95500 (p=77100), loss: 49.426788\n",
      "iter 96000 (p=89600), loss: 49.545048\n",
      "iter 96500 (p=102100), loss: 49.256462\n",
      "iter 97000 (p=114600), loss: 49.165468\n",
      "iter 97500 (p=127100), loss: 49.361796\n",
      "iter 98000 (p=139600), loss: 49.420088\n",
      "iter 98500 (p=7700), loss: 49.835335\n",
      "iter 99000 (p=20200), loss: 49.876554\n",
      "iter 99500 (p=32700), loss: 50.160876\n",
      "----\n",
      " e Alited and now:\n",
      "‘I the torse, heve a\n",
      "seepes that’s I dinter t the Que s! ut a fir-anste pat that ang---but’s.’   Say in hast fithid ment doke\n",
      "to Ane, fouw wher mustely said\n",
      "fust, thens,\n",
      "of, whh, and \n",
      "----\n",
      "iter 100000 (p=45200), loss: 50.263772\n",
      "iter 100500 (p=57700), loss: 49.914889\n",
      "iter 101000 (p=70200), loss: 49.681668\n",
      "iter 101500 (p=82700), loss: 49.178284\n",
      "iter 102000 (p=95200), loss: 49.248089\n",
      "iter 102500 (p=107700), loss: 48.918209\n",
      "iter 103000 (p=120200), loss: 48.974136\n",
      "iter 103500 (p=132700), loss: 48.996527\n",
      "iter 104000 (p=800), loss: 49.345407\n",
      "iter 104500 (p=13300), loss: 49.580251\n",
      "iter 105000 (p=25800), loss: 50.245953\n",
      "iter 105500 (p=38300), loss: 50.058471\n",
      "iter 106000 (p=50800), loss: 50.112758\n",
      "iter 106500 (p=63300), loss: 49.731468\n",
      "iter 107000 (p=75800), loss: 49.135158\n",
      "iter 107500 (p=88300), loss: 49.201768\n",
      "iter 108000 (p=100800), loss: 49.056456\n",
      "iter 108500 (p=113300), loss: 48.826242\n",
      "iter 109000 (p=125800), loss: 49.166312\n",
      "iter 109500 (p=138300), loss: 48.992538\n",
      "----\n",
      " y chamodith of the Mat ut Cand tuinve hersere, and ne the phe wilt, afgs out you mand geesched the negef the freeple.\n",
      "\n",
      "abis, a mad upearly une at waterensing!\n",
      "\n",
      "*\n",
      "   I waytourd thatily onss saipthitsin \n",
      "----\n",
      "iter 110000 (p=6400), loss: 49.574291\n",
      "iter 110500 (p=18900), loss: 49.516609\n",
      "iter 111000 (p=31400), loss: 49.913460\n",
      "iter 111500 (p=43900), loss: 50.071223\n",
      "iter 112000 (p=56400), loss: 49.680569\n",
      "iter 112500 (p=68900), loss: 49.566099\n",
      "iter 113000 (p=81400), loss: 48.992797\n",
      "iter 113500 (p=93900), loss: 49.078066\n",
      "iter 114000 (p=106400), loss: 48.548042\n",
      "iter 114500 (p=118900), loss: 48.652144\n",
      "iter 115000 (p=131400), loss: 48.675977\n",
      "iter 115500 (p=143900), loss: 48.946519\n",
      "iter 116000 (p=12000), loss: 49.236175\n",
      "iter 116500 (p=24500), loss: 49.857662\n",
      "iter 117000 (p=37000), loss: 49.830372\n",
      "iter 117500 (p=49500), loss: 49.897738\n",
      "iter 118000 (p=62000), loss: 49.602829\n",
      "iter 118500 (p=74500), loss: 48.977941\n",
      "iter 119000 (p=87000), loss: 48.933896\n",
      "iter 119500 (p=99500), loss: 48.796146\n",
      "----\n",
      " the!\n",
      "\n",
      "Thomecurteper in fy said, ‘and seined i, now---HAs\n",
      "‘You to agmeid at oneid. ‘I nary as),\n",
      "\n",
      "\n",
      " *   plaing on to kyorbeo he ghee cammusbure bechent wit so thoad sonsing in senmno plponkth!’ sall me  \n",
      "----\n",
      "iter 120000 (p=112000), loss: 48.757179\n",
      "iter 120500 (p=124500), loss: 48.922934\n",
      "iter 121000 (p=137000), loss: 48.666832\n",
      "iter 121500 (p=5100), loss: 49.423073\n",
      "iter 122000 (p=17600), loss: 49.272627\n",
      "iter 122500 (p=30100), loss: 49.509902\n",
      "iter 123000 (p=42600), loss: 49.964765\n",
      "iter 123500 (p=55100), loss: 49.451750\n",
      "iter 124000 (p=67600), loss: 49.392029\n",
      "iter 124500 (p=80100), loss: 48.820065\n",
      "iter 125000 (p=92600), loss: 48.910919\n",
      "iter 125500 (p=105100), loss: 48.381846\n",
      "iter 126000 (p=117600), loss: 48.298619\n",
      "iter 126500 (p=130100), loss: 48.519775\n",
      "iter 127000 (p=142600), loss: 48.665879\n",
      "iter 127500 (p=10700), loss: 49.100465\n",
      "iter 128000 (p=23200), loss: 49.415386\n",
      "iter 128500 (p=35700), loss: 49.537592\n",
      "iter 129000 (p=48200), loss: 49.605259\n",
      "iter 129500 (p=60700), loss: 49.421420\n",
      "----\n",
      " me to the coped verteid arind a des: and, ang fir it\n",
      "that or the hithen all the Kiepkinus,’ semed to wit ose, as walln’t of itt don, creife.\n",
      "         pyoud it\n",
      "haint wo a gens the Kine meY weat--’ to d \n",
      "----\n",
      "iter 130000 (p=73200), loss: 48.853128\n",
      "iter 130500 (p=85700), loss: 48.556616\n",
      "iter 131000 (p=98200), loss: 48.508877\n",
      "iter 131500 (p=110700), loss: 48.430872\n",
      "iter 132000 (p=123200), loss: 48.689715\n",
      "iter 132500 (p=135700), loss: 48.464554\n",
      "iter 133000 (p=3800), loss: 49.013848\n",
      "iter 133500 (p=16300), loss: 49.159894\n",
      "iter 134000 (p=28800), loss: 49.376128\n",
      "iter 134500 (p=41300), loss: 49.815060\n",
      "iter 135000 (p=53800), loss: 49.290785\n",
      "iter 135500 (p=66300), loss: 49.230883\n",
      "iter 136000 (p=78800), loss: 48.629541\n",
      "iter 136500 (p=91300), loss: 48.605668\n",
      "iter 137000 (p=103800), loss: 48.268001\n",
      "iter 137500 (p=116300), loss: 48.032005\n",
      "iter 138000 (p=128800), loss: 48.343291\n",
      "iter 138500 (p=141300), loss: 48.470341\n",
      "iter 139000 (p=9400), loss: 48.913951\n",
      "iter 139500 (p=21900), loss: 49.120336\n",
      "----\n",
      " ce you taid af mat, and the hardet wise cow a sould,\n",
      "re ap paid.\n",
      "             a Cas her fike did-ychast it fean, [ and, ang a her it thy ENI’\n",
      "shing\n",
      "on the have so\n",
      "she sap tike the\n",
      "Fittle trared veveis \n",
      "----\n",
      "iter 140000 (p=34400), loss: 49.350625\n",
      "iter 140500 (p=46900), loss: 49.507714\n",
      "iter 141000 (p=59400), loss: 49.173487\n",
      "iter 141500 (p=71900), loss: 48.719607\n",
      "iter 142000 (p=84400), loss: 48.242407\n",
      "iter 142500 (p=96900), loss: 48.313932\n",
      "iter 143000 (p=109400), loss: 48.158924\n",
      "iter 143500 (p=121900), loss: 48.460602\n",
      "iter 144000 (p=134400), loss: 48.179881\n",
      "iter 144500 (p=2500), loss: 48.596497\n",
      "iter 145000 (p=15000), loss: 48.771987\n",
      "iter 145500 (p=27500), loss: 49.349352\n",
      "iter 146000 (p=40000), loss: 49.522437\n",
      "iter 146500 (p=52500), loss: 49.229557\n",
      "iter 147000 (p=65000), loss: 49.055457\n",
      "iter 147500 (p=77500), loss: 48.334963\n",
      "iter 148000 (p=90000), loss: 48.429258\n",
      "iter 148500 (p=102500), loss: 48.121840\n",
      "iter 149000 (p=115000), loss: 47.940058\n",
      "iter 149500 (p=127500), loss: 48.207344\n",
      "----\n",
      "  to hey in the fand and\n",
      "couh rured about in lestifed spearser meyn--bressorote of preod, the anrf!’ ‘now at bickt not and uny that,’ the I theave, mupped?’\n",
      "\n",
      "‘Why -uw, ‘It annt mele abithruvess trarzay \n",
      "----\n",
      "iter 150000 (p=140000), loss: 48.287134\n",
      "iter 150500 (p=8100), loss: 48.770358\n",
      "iter 151000 (p=20600), loss: 48.800152\n",
      "iter 151500 (p=33100), loss: 49.153348\n",
      "iter 152000 (p=45600), loss: 49.203398\n",
      "iter 152500 (p=58100), loss: 48.936229\n",
      "iter 153000 (p=70600), loss: 48.620938\n",
      "iter 153500 (p=83100), loss: 48.139136\n"
     ]
    }
   ],
   "source": [
    "n,p = 0,0\n",
    "\n",
    "mWax, mWas, mWay = np.zeros_like(Wax), np.zeros_like(Was), np.zeros_like(Way)\n",
    "\n",
    "mb1, mb2 = np.zeros_like(b1), np.zeros_like(b2)\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_length)*sequence\n",
    "\n",
    "while p <1000000:\n",
    "    # Prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+sequence+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden, 1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "\n",
    "    # In each step we unroll the RNN for seq_length cells, and present it with\n",
    "    # sequence length inputs and sequence length  target outputs to learn.\n",
    "    inputs = [char_int[ch] for ch in data[p:p+sequence]]\n",
    "    targets = [char_int[ch] for ch in data[p+1:p+sequence+1]]\n",
    "\n",
    "    # Sample from the model now and then.\n",
    "    if n % 10000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(int_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # Forward sequence length characters through the net and fetch gradient\n",
    "    loss, dWax, dWas, dWay, db1, db2,hprev = calculateLoss(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 500 == 0: print('iter %d (p=%d), loss: %f' % (n, p, smooth_loss))\n",
    "\n",
    "    # Perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wax, Was, Way, b1, b2],[dWax, dWas, dWay, db1, db2],[mWax, mWas, mWay,mb1,mb2]):\n",
    "        mem += dparam * dparam\n",
    "        param += -0.1 * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    p += sequence\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
